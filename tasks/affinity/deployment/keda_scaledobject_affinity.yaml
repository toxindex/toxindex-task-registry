# KEDA ScaledObject for Celery Affinity Worker
# Watches the Redis queue and scales from 0 when tasks arrive
#
# Prerequisites:
# 1. Install KEDA in the cluster:
#    kubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.0/keda-2.13.0.yaml
#
# 2. Create the Redis connection secret (TriggerAuthentication):
#    kubectl apply -f keda_trigger_auth_affinity.yaml
#
# 3. Apply this ScaledObject:
#    kubectl apply -f keda_scaledobject_affinity.yaml
#
# Note: When using KEDA, you can remove or keep the HPA. KEDA will manage scaling
# including scale-to-zero. If you keep HPA with minReplicas: 0, KEDA handles 0→1
# and HPA handles 1→N scaling.

apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: celery-worker-affinity-scaledobject
  namespace: toxindex-app
spec:
  scaleTargetRef:
    name: celery-worker-affinity
    kind: Deployment

  # Polling interval - how often KEDA checks the queue
  pollingInterval: 15  # seconds

  # Cooldown period before scaling down after queue is empty
  cooldownPeriod: 60  # 1 minute - Prometheus monitors active tasks so we can scale down quickly

  # Min/max replicas
  # Note: idleReplicaCount removed - it must be < minReplicaCount (for scale-to-zero)
  # Since we want 1 warm pod always, we just set minReplicaCount: 1
  minReplicaCount: 1   # Always keep at least 1 warm
  maxReplicaCount: 16  # Max pods (L4 GPUs)

  # Advanced scaling behavior
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          # Prometheus trigger monitors active tasks, so we can scale down faster
          stabilizationWindowSeconds: 60  # 1 minute
          policies:
          - type: Pods
            value: 4           # Scale down 4 pods at a time
            periodSeconds: 30  # Every 30 seconds
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
          - type: Pods
            value: 4           # Scale up aggressively (4 pods at a time)
            periodSeconds: 15

  triggers:
  # Trigger 1: Watch pending GPU subtasks in Redis queue
  - type: redis
    metadata:
      address: "10.238.5.54:6379"
      listName: "affinity_gpu"    # Celery queue for GPU subtasks only
      listLength: "1"             # Scale when >= 1 pending task
      databaseIndex: "0"

  # Trigger 2: Watch active tasks via Prometheus (scraping Flower metrics)
  # This prevents scale-down while workers are processing tasks
  # Architecture: Flower -> Prometheus (scrapes /metrics) -> KEDA (queries /api/v1/query)
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.toxindex-app.svc.cluster.local:9090
      metricName: celery_active_tasks
      # Sum active tasks across GPU affinity workers only (exclude affinity-cpu orchestrators)
      query: sum(flower_worker_number_of_currently_executing_tasks{worker=~".*affinity.*",worker!~".*affinity-cpu.*"}) or vector(0)
      threshold: "1"              # Keep pods alive when >= 1 active task
